{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Solution: VAE for Image Reconstruction and Generation\n",
    "\n",
    "**Student:** Rimoldi\n",
    "\n",
    "**Exam Session:** July 15, 2025\n",
    "\n",
    "This notebook implements the proposed solution for the exam. The goal is to design a deep neural network capable of encoding images into a probabilistic latent space, then reconstructing or generating new images, closely following the structure of the provided example code and the numerical order of the exam sheet.\n",
    "\n",
    "Notes :\n",
    "- This notebook takes around 6 minutes to run.\n",
    "- I underlined the differences between the exam in the following code like this: <font color=\"red\">**CHANGE**</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (4.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pablo\\documents\\github\\deeplearningexam_vae\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, ops, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces the dataset loading process, utilizing the requests library to download the necessary data from the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/PaRi29/DeepLearningExam_VAE/main/assets/input_data.pkl...\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/PaRi29/DeepLearningExam_VAE/main/assets/input_data.pkl\"\n",
    "print(f\"Downloading data from {url}...\")\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "with open(\"input_data.pkl\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: MODEL\n",
    "\n",
    "**Model Choice:** Variational Autoencoder (VAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2: INPUT AND PREPROCESSING\n",
    "\n",
    "Normalization: Since we are working with grayscale images, pixel values are normalized from the [0, 255] range to the [0, 1] range by dividing each value by 255.0. This process helps stabilize and speed up model training.\n",
    "\n",
    "Input Format (Shape): An additional channel dimension is added to the images (since they are grayscale, there is only one channel). The resulting input shape is (batch_size, 28, 28, 1), which is required by convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Training data shape: (4900, 28, 28, 1)\n",
      "Validation data shape: (1400, 28, 28, 1)\n",
      "Test data shape: (700, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"input_data.pkl\", \"rb\") as f:\n",
    "        dd = pickle.load(f)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    data = dd['data']\n",
    "    labels = dd['labels']\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "def preprocess_images(images):\n",
    "    images = images.astype(\"float32\") / 255.0\n",
    "    images = np.expand_dims(images, axis=-1)\n",
    "    return images\n",
    "\n",
    "data = preprocess_images(data)\n",
    "\n",
    "# Create train, validation, and test splits\n",
    "test_fraction = 0.1\n",
    "val_fraction = 0.2\n",
    "\n",
    "num_samples = data.shape[0]\n",
    "num_test = int(num_samples * test_fraction)\n",
    "num_val = int(num_samples * val_fraction)\n",
    "num_train = num_samples - num_val - num_test\n",
    "\n",
    "# Shuffle the data before splitting\n",
    "indices = np.arange(num_samples)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_test = data[:num_test]\n",
    "y_test = labels[:num_test]\n",
    "\n",
    "x_val = data[num_test:num_test+num_val]\n",
    "y_val = labels[num_test:num_test+num_val]\n",
    "\n",
    "x_train = data[num_test+num_val:]\n",
    "y_train = labels[num_test+num_val:]\n",
    "\n",
    "INPUT_SHAPE = x_train.shape[1:]\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Validation data shape: {x_val.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: MODEL CONFIGURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Model Composition\n",
    "The VAE consists of an Encoder, a Decoder, and a specialized `Sampling` layer. \n",
    "**Conceptual Outline:** `Input Image` -> **[Encoder]** -> `Latent Distribution` -> **[Sampling]** -> `Latent Vector` -> **[Decoder]** -> `Reconstructed Image`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Image Projection (Encoder)\n",
    "The Encoder is a CNN that maps an image to a probabilistic latent space. It reduces the image's dimensionality through convolutional and pooling layers, finally outputting the mean (`z_mean`) and log-variance (`z_log_var`) of the learned distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Image Reconstruction (Decoder)\n",
    "The Decoder is a deconvolutional network, symmetric to the encoder. It takes a latent vector `z` (sampled from the distribution defined by `z_mean` and `z_log_var`), upsamples it through transposed convolutions (or upsampling layers), and reconstructs the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Generation of New Images\n",
    "To generate new images, we discard the encoder and use only the trained decoder. A random vector `z` is sampled from a standard normal distribution N(0,1) and passed to the decoder, which then generates a new, coherent image that was not in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Hyperparameters\n",
    "Key hyperparameters to tune include `learning_rate`, `batch_size`, `latent_dim` (the size of the latent space), and `kl_reg` (the weight of the KL divergence loss). A random search is performed to find a good combination of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z (reparameterization trick).\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = ops.shape(z_mean)[0]\n",
    "        dim = ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal(shape=(batch, dim))\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def vae_enc(input_shape, latent_dim, filter_base=32, dense_units=16):\n",
    "    \"\"\"Encoder model (Projection), configurable via hyperparameters.\"\"\"\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(filter_base, 3, activation=\"relu\", padding=\"same\")(encoder_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2, padding=\"same\")(x)  # 28x28 -> 14x14\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.Conv2D(filter_base * 2, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2, padding=\"same\")(x)  # 14x14 -> 7x7\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    return encoder\n",
    "\n",
    "def vae_dec(latent_dim, filter_base=32, dense_units=16):\n",
    "    \"\"\"Decoder model (Reconstruction), configurable via hyperparameters.\"\"\"\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    \n",
    "    # Initial part to prepare for reconstruction\n",
    "    x = layers.Dense(7 * 7 * (filter_base * 2), activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((7, 7, filter_base * 2))(x)\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.UpSampling2D(2)(x)  # 7x7 -> 14x14\n",
    "    x = layers.Conv2D(filter_base * 2, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.UpSampling2D(2)(x)  # 14x14 -> 28x28\n",
    "    x = layers.Conv2D(filter_base, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    decoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    decoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    return decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid and search function for hyperparameters (Point 3e)\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 1e-3],\n",
    "    'batch_size':    [64, 128],\n",
    "    'latent_dim':    [2, 8],\n",
    "    'kl_reg':        [0.5, 1.0, 2.0],\n",
    "    'optimizer':     ['adam', 'rmsprop'],\n",
    "    'filter_base':   [16, 32],\n",
    "    'dense_units':   [16, 32],\n",
    "    'patience':      [2, 3, 5],\n",
    "    'min_delta':     [0.0, 1e-4, 1e-3],\n",
    "    'restore_best_weights': [True, False],\n",
    "    'monitor':       ['val_loss', 'val_reconstruction_loss']\n",
    "}\n",
    "num_samples = 5 # Number of random combinations to test\n",
    "\n",
    "\n",
    "def random_search_vae(VAE_MODEL, INPUT_SHAPE, param_grid, x_train, x_val, samples=num_samples):\n",
    "    \"\"\"\n",
    "    Performs a random search over the defined parameter grid.\n",
    "    \n",
    "    Args:\n",
    "        VAE_MODEL: The VAE model class (which must be defined).\n",
    "        INPUT_SHAPE: The shape of the input images.\n",
    "        param_grid: A dictionary of hyperparameters to search over.\n",
    "        x_train: Training data.\n",
    "        x_val: Validation data.\n",
    "        samples: The number of random combinations to try.\n",
    "    \n",
    "    Returns:\n",
    "        The dictionary containing the best hyperparameter configuration found.\n",
    "    \"\"\"\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    sampled_combos = random.sample(combos, min(samples, len(combos)))\n",
    "    configs = [dict(zip(param_grid.keys(), c)) for c in sampled_combos]\n",
    "    best_val_loss = np.inf\n",
    "    best_cfg = None\n",
    "\n",
    "    for idx, cfg in enumerate(configs):\n",
    "        print(f\"\\n=== Training config {idx+1}/{len(configs)} ===\")\n",
    "        print(\"Config:\", cfg)\n",
    "        \n",
    "        # Build model with current config\n",
    "        encoder = vae_enc(INPUT_SHAPE, cfg['latent_dim'], cfg['filter_base'], cfg['dense_units'])\n",
    "        decoder = vae_dec(cfg['latent_dim'], cfg['filter_base'], cfg['dense_units'])\n",
    "        vae_model = VAE_MODEL(encoder, decoder, reg=cfg['kl_reg'])\n",
    "        \n",
    "        # Select and configure optimizer\n",
    "        if cfg['optimizer'] == 'adam':\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=cfg['learning_rate'])\n",
    "        else: # rmsprop\n",
    "            optimizer = keras.optimizers.RMSprop(learning_rate=cfg['learning_rate'])\n",
    "            \n",
    "        vae_model.compile(optimizer=optimizer)\n",
    "        \n",
    "        # Define Early Stopping callback with all relevant hyperparameters\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor=cfg['monitor'],\n",
    "            patience=cfg['patience'],\n",
    "            min_delta=cfg['min_delta'],\n",
    "            verbose=1,\n",
    "            restore_best_weights=cfg['restore_best_weights'],\n",
    "            mode='min'\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        history = vae_model.fit(\n",
    "            x_train, x_train,\n",
    "            epochs=50, # Set a high number; Early Stopping will terminate training\n",
    "            batch_size=cfg['batch_size'],\n",
    "            validation_data=(x_val, x_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1 # Suppress epoch-by-epoch output for cleaner search\n",
    "        )\n",
    "        \n",
    "        # Get the best validation loss from this run (thanks to restore_best_weights)\n",
    "        # Use the same metric as monitored\n",
    "        monitor_metric = cfg['monitor']\n",
    "        if monitor_metric in history.history:\n",
    "            final_val_loss = min(history.history[monitor_metric])\n",
    "        else:\n",
    "            # fallback to val_loss if custom metric not found\n",
    "            final_val_loss = min(history.history['val_loss'])\n",
    "        print(f\"Best {monitor_metric} for this config: {final_val_loss:.4f}\")\n",
    "        \n",
    "        if final_val_loss < best_val_loss:\n",
    "            best_val_loss = final_val_loss\n",
    "            best_cfg = cfg\n",
    "            print(f\"*** New best model found! ***\")\n",
    "\n",
    "    print(\"\\n=== Best Results Summary ===\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"Best configuration:\", best_cfg)\n",
    "    return best_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: OUTPUT\n",
    "\n",
    "The output layer of the decoder is a `Conv2D` layer designed as follows:\n",
    "- **Filters:** `1`. This is because the output images are grayscale and have only one channel.\n",
    "- **Activation Function:** `sigmoid`. The input images were normalized to the range [0, 1]. The sigmoid function outputs values in this same [0, 1] range, making it the natural choice for the final layer. This ensures the reconstructed pixel values are on the same scale as the input, which is essential for calculating the reconstruction loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LOSS\n",
    "\n",
    "CHANGE with respect to exam: In the exam, I **mistakenly used Mean Absolute Error (MAE)** as the reconstruction loss.  \n",
    "While MAE is sometimes used in **standard autoencoders** due to its robustness to outliers, it is **not the canonical choice for Variational Autoencoders (VAEs)**.\n",
    "\n",
    "The loss function for a **VAE** consists of two main components, which reflect its **probabilistic generative nature**:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Reconstruction Loss**  \n",
    "For VAEs, the reconstruction term is derived from the assumption that pixels are generated from a Gaussian distribution. Under this assumption, the appropriate reconstruction loss is the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{recon}} = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{x}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ is the original pixel value,\n",
    "- $\\hat{x}_i$ is the reconstructed pixel value,\n",
    "- $N$ is the total number of pixels.\n",
    "\n",
    "\n",
    "**2. Kullback-Leibler (KL) Divergence**  \n",
    "This term ensures that the latent space distribution learned by the encoder remains close to a prior distribution (usually a standard normal $\\mathcal{N}(0, I)$):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{KL}} = -\\frac{1}{2} \\sum_{j=1}^{d} \\left(1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_j$ and $\\sigma_j$ are the predicted mean and standard deviation for latent dimension $j$,\n",
    "- $d$ is the dimensionality of the latent space.\n",
    "\n",
    "This term is **crucial** for enabling the model to generalize and generate new data by sampling from the latent space.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Total Loss Function**\n",
    "\n",
    "The final VAE loss combines the two components:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{recon}} + \\beta \\cdot \\mathcal{L}_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\beta$ is a hyperparameter that balances reconstruction accuracy with latent space regularization.  \n",
    "  In the standard VAE, $\\beta = 1$, but it can be tuned (e.g., in $\\beta$-VAEs) to control disentanglement.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**  \n",
    "My initial response mistakenly applied a loss formulation more suited to standard autoencoders.  \n",
    "A correct VAE implementation must:\n",
    "- Use **MSE** for reconstruction (derived from Gaussian likelihood),\n",
    "- Include the **KL divergence** for latent regularization,\n",
    "- Combine both in a total loss that supports both reconstruction and generative capabilities.\n",
    "\n",
    "This correction reflects a deeper understanding of **why** these loss terms are used in VAEs and how they relate to the model’s probabilistic foundations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, reg=1.0, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.reg = reg\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data tuple. 'x' will be your input tensor.\n",
    "        # In this VAE case, both elements of the tuple are the same,\n",
    "        # so we can assign the first to 'x' and ignore the second.\n",
    "        x, _ = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Pass the single input tensor 'x' to the encoder\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # Also, ensure you use 'x' to calculate the reconstruction loss\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(tf.square(x - reconstruction), axis=(1, 2, 3))\n",
    "            )\n",
    "\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "\n",
    "            total_loss = reconstruction_loss + self.reg * kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        } \n",
    "       \n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var, z = self.encoder(data, training=False)\n",
    "        reconstruction = self.decoder(z, training=False)\n",
    "\n",
    "        # Calculate reconstruction loss directly\n",
    "        reconstruction_loss = ops.mean(\n",
    "            ops.sum(ops.square(data - reconstruction), axis=(1, 2))\n",
    "        )\n",
    "        \n",
    "        kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "        kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "        \n",
    "        total_loss = reconstruction_loss + self.reg * kl_loss\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        _, _, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: MODEL EVALUATION\n",
    "\n",
    "The model is evaluated through a comprehensive process that includes hyperparameter tuning, final training, and both quantitative and qualitative assessment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Search Execution\n",
    "First, we execute the random search defined in Point 3e to find the best hyperparameters using a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training config 1/5 ===\n",
      "Config: {'learning_rate': 0.0001, 'batch_size': 128, 'latent_dim': 2, 'kl_reg': 0.5, 'optimizer': 'rmsprop', 'filter_base': 16, 'dense_units': 32, 'patience': 2, 'min_delta': 0.001, 'restore_best_weights': True, 'monitor': 'val_loss'}\n",
      "Epoch 1/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - kl_loss: 0.0716 - loss: 129.2583 - reconstruction_loss: 129.2225 - val_kl_loss: 1.1114 - val_loss: 115.0126 - val_reconstruction_loss: 114.4569\n",
      "Epoch 2/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 1.5502 - loss: 110.7568 - reconstruction_loss: 109.9817 - val_kl_loss: 2.7256 - val_loss: 100.2619 - val_reconstruction_loss: 98.8991\n",
      "Epoch 3/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 3.0823 - loss: 95.7532 - reconstruction_loss: 94.2121 - val_kl_loss: 4.7259 - val_loss: 85.4664 - val_reconstruction_loss: 83.1035\n",
      "Epoch 4/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - kl_loss: 4.9986 - loss: 81.3759 - reconstruction_loss: 78.8767 - val_kl_loss: 5.0468 - val_loss: 73.0543 - val_reconstruction_loss: 70.5309\n",
      "Epoch 5/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 5.5257 - loss: 69.7831 - reconstruction_loss: 67.0202 - val_kl_loss: 5.9679 - val_loss: 61.7409 - val_reconstruction_loss: 58.7569\n",
      "Epoch 6/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 6.1291 - loss: 58.9277 - reconstruction_loss: 55.8631 - val_kl_loss: 6.1345 - val_loss: 53.4848 - val_reconstruction_loss: 50.4175\n",
      "Epoch 7/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - kl_loss: 6.6942 - loss: 51.5812 - reconstruction_loss: 48.2341 - val_kl_loss: 6.5282 - val_loss: 48.4169 - val_reconstruction_loss: 45.1528\n",
      "Epoch 8/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 6.8414 - loss: 47.3182 - reconstruction_loss: 43.8975 - val_kl_loss: 6.9573 - val_loss: 45.3389 - val_reconstruction_loss: 41.8602\n",
      "Epoch 9/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 6.7407 - loss: 44.4111 - reconstruction_loss: 41.0407 - val_kl_loss: 6.6627 - val_loss: 43.0111 - val_reconstruction_loss: 39.6798\n",
      "Epoch 10/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - kl_loss: 6.5243 - loss: 42.5879 - reconstruction_loss: 39.3258 - val_kl_loss: 6.0764 - val_loss: 42.1298 - val_reconstruction_loss: 39.0916\n",
      "Epoch 11/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 6.3892 - loss: 41.4537 - reconstruction_loss: 38.2591 - val_kl_loss: 6.4523 - val_loss: 40.7755 - val_reconstruction_loss: 37.5494\n",
      "Epoch 12/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 6.2687 - loss: 40.3211 - reconstruction_loss: 37.1867 - val_kl_loss: 5.9609 - val_loss: 39.7746 - val_reconstruction_loss: 36.7941\n",
      "Epoch 13/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - kl_loss: 6.1105 - loss: 39.5017 - reconstruction_loss: 36.4465 - val_kl_loss: 6.0541 - val_loss: 39.0161 - val_reconstruction_loss: 35.9890\n",
      "Epoch 14/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - kl_loss: 6.0417 - loss: 38.8247 - reconstruction_loss: 35.8039 - val_kl_loss: 6.0536 - val_loss: 38.4558 - val_reconstruction_loss: 35.4290\n",
      "Epoch 15/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 5.9604 - loss: 38.3853 - reconstruction_loss: 35.4051 - val_kl_loss: 5.6168 - val_loss: 38.7244 - val_reconstruction_loss: 35.9160\n",
      "Epoch 16/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - kl_loss: 5.8913 - loss: 37.9786 - reconstruction_loss: 35.0329 - val_kl_loss: 5.9658 - val_loss: 37.9018 - val_reconstruction_loss: 34.9189\n",
      "Epoch 17/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - kl_loss: 5.8748 - loss: 37.4483 - reconstruction_loss: 34.5109 - val_kl_loss: 5.9440 - val_loss: 37.4600 - val_reconstruction_loss: 34.4880\n",
      "Epoch 18/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - kl_loss: 5.8374 - loss: 37.3714 - reconstruction_loss: 34.4527 - val_kl_loss: 5.7930 - val_loss: 36.9251 - val_reconstruction_loss: 34.0286\n",
      "Epoch 19/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 5.7776 - loss: 36.9234 - reconstruction_loss: 34.0346 - val_kl_loss: 5.6638 - val_loss: 36.6143 - val_reconstruction_loss: 33.7825\n",
      "Epoch 20/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 5.7478 - loss: 36.5941 - reconstruction_loss: 33.7202 - val_kl_loss: 5.8239 - val_loss: 36.4985 - val_reconstruction_loss: 33.5865\n",
      "Epoch 21/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - kl_loss: 5.7372 - loss: 36.2585 - reconstruction_loss: 33.3899 - val_kl_loss: 5.4893 - val_loss: 37.2615 - val_reconstruction_loss: 34.5169\n",
      "Epoch 22/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 5.7182 - loss: 36.3096 - reconstruction_loss: 33.4505 - val_kl_loss: 5.6347 - val_loss: 36.1309 - val_reconstruction_loss: 33.3135\n",
      "Epoch 23/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - kl_loss: 5.6807 - loss: 35.8568 - reconstruction_loss: 33.0164 - val_kl_loss: 5.7077 - val_loss: 35.8461 - val_reconstruction_loss: 32.9923\n",
      "Epoch 24/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - kl_loss: 5.6488 - loss: 35.6323 - reconstruction_loss: 32.8080 - val_kl_loss: 5.7724 - val_loss: 36.1515 - val_reconstruction_loss: 33.2653\n",
      "Epoch 25/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 5.6379 - loss: 35.3877 - reconstruction_loss: 32.5687 - val_kl_loss: 5.4051 - val_loss: 35.9010 - val_reconstruction_loss: 33.1984\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Best val_loss for this config: 35.8461\n",
      "*** New best model found! ***\n",
      "\n",
      "=== Training config 2/5 ===\n",
      "Config: {'learning_rate': 0.001, 'batch_size': 64, 'latent_dim': 2, 'kl_reg': 2.0, 'optimizer': 'adam', 'filter_base': 32, 'dense_units': 16, 'patience': 5, 'min_delta': 0.0, 'restore_best_weights': True, 'monitor': 'val_reconstruction_loss'}\n",
      "Epoch 1/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - kl_loss: 0.0979 - loss: 96.2979 - reconstruction_loss: 96.1021 - val_kl_loss: 0.4185 - val_loss: 68.2562 - val_reconstruction_loss: 67.4191\n",
      "Epoch 2/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 0.9506 - loss: 66.6091 - reconstruction_loss: 64.7079 - val_kl_loss: 0.8136 - val_loss: 64.0657 - val_reconstruction_loss: 62.4384\n",
      "Epoch 3/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 1.6662 - loss: 62.0223 - reconstruction_loss: 58.6900 - val_kl_loss: 2.3771 - val_loss: 56.9214 - val_reconstruction_loss: 52.1672\n",
      "Epoch 4/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - kl_loss: 2.5548 - loss: 55.0463 - reconstruction_loss: 49.9366 - val_kl_loss: 2.0219 - val_loss: 53.7889 - val_reconstruction_loss: 49.7451\n",
      "Epoch 5/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 2.7232 - loss: 52.9782 - reconstruction_loss: 47.5318 - val_kl_loss: 2.4687 - val_loss: 51.7313 - val_reconstruction_loss: 46.7938\n",
      "Epoch 6/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - kl_loss: 2.7473 - loss: 50.9493 - reconstruction_loss: 45.4546 - val_kl_loss: 2.4005 - val_loss: 49.9964 - val_reconstruction_loss: 45.1953\n",
      "Epoch 7/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - kl_loss: 2.8315 - loss: 49.0394 - reconstruction_loss: 43.3764 - val_kl_loss: 3.2033 - val_loss: 43.8137 - val_reconstruction_loss: 37.4071\n",
      "Epoch 8/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 3.4828 - loss: 42.0926 - reconstruction_loss: 35.1270 - val_kl_loss: 3.3030 - val_loss: 39.9715 - val_reconstruction_loss: 33.3654\n",
      "Epoch 9/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 3.4242 - loss: 38.7739 - reconstruction_loss: 31.9255 - val_kl_loss: 3.5434 - val_loss: 38.4678 - val_reconstruction_loss: 31.3809\n",
      "Epoch 10/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 3.5180 - loss: 37.8578 - reconstruction_loss: 30.8217 - val_kl_loss: 3.4441 - val_loss: 37.9811 - val_reconstruction_loss: 31.0929\n",
      "Epoch 11/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.4793 - loss: 37.3315 - reconstruction_loss: 30.3729 - val_kl_loss: 3.5588 - val_loss: 37.1014 - val_reconstruction_loss: 29.9838\n",
      "Epoch 12/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.5886 - loss: 36.7709 - reconstruction_loss: 29.5937 - val_kl_loss: 3.4931 - val_loss: 36.4938 - val_reconstruction_loss: 29.5076\n",
      "Epoch 13/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - kl_loss: 3.5750 - loss: 36.1272 - reconstruction_loss: 28.9771 - val_kl_loss: 3.5239 - val_loss: 36.4680 - val_reconstruction_loss: 29.4201\n",
      "Epoch 14/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - kl_loss: 3.5855 - loss: 36.1626 - reconstruction_loss: 28.9916 - val_kl_loss: 3.5640 - val_loss: 36.4008 - val_reconstruction_loss: 29.2729\n",
      "Epoch 15/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.6037 - loss: 35.7246 - reconstruction_loss: 28.5173 - val_kl_loss: 3.5387 - val_loss: 35.9296 - val_reconstruction_loss: 28.8522\n",
      "Epoch 16/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.6052 - loss: 35.5286 - reconstruction_loss: 28.3183 - val_kl_loss: 3.5194 - val_loss: 36.8550 - val_reconstruction_loss: 29.8163\n",
      "Epoch 17/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.6351 - loss: 35.3854 - reconstruction_loss: 28.1151 - val_kl_loss: 3.6335 - val_loss: 36.5049 - val_reconstruction_loss: 29.2379\n",
      "Epoch 18/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - kl_loss: 3.6201 - loss: 35.6310 - reconstruction_loss: 28.3908 - val_kl_loss: 3.6522 - val_loss: 35.6061 - val_reconstruction_loss: 28.3016\n",
      "Epoch 19/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 3.6352 - loss: 35.0321 - reconstruction_loss: 27.7617 - val_kl_loss: 3.5617 - val_loss: 36.0283 - val_reconstruction_loss: 28.9050\n",
      "Epoch 20/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.6608 - loss: 35.2475 - reconstruction_loss: 27.9259 - val_kl_loss: 3.5987 - val_loss: 35.4048 - val_reconstruction_loss: 28.2074\n",
      "Epoch 21/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - kl_loss: 3.6304 - loss: 34.7910 - reconstruction_loss: 27.5301 - val_kl_loss: 3.7037 - val_loss: 36.1856 - val_reconstruction_loss: 28.7782\n",
      "Epoch 22/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - kl_loss: 3.7023 - loss: 35.1476 - reconstruction_loss: 27.7430 - val_kl_loss: 3.6552 - val_loss: 35.6974 - val_reconstruction_loss: 28.3869\n",
      "Epoch 23/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 3.6368 - loss: 34.6595 - reconstruction_loss: 27.3859 - val_kl_loss: 3.6820 - val_loss: 35.4982 - val_reconstruction_loss: 28.1342\n",
      "Epoch 24/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 3.6878 - loss: 34.5369 - reconstruction_loss: 27.1612 - val_kl_loss: 3.6644 - val_loss: 35.7983 - val_reconstruction_loss: 28.4695\n",
      "Epoch 25/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.6545 - loss: 34.6710 - reconstruction_loss: 27.3620 - val_kl_loss: 3.6131 - val_loss: 35.7466 - val_reconstruction_loss: 28.5205\n",
      "Epoch 26/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.6606 - loss: 34.5318 - reconstruction_loss: 27.2105 - val_kl_loss: 3.7968 - val_loss: 34.8475 - val_reconstruction_loss: 27.2538\n",
      "Epoch 27/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - kl_loss: 3.6885 - loss: 34.3422 - reconstruction_loss: 26.9652 - val_kl_loss: 3.7214 - val_loss: 35.1110 - val_reconstruction_loss: 27.6682\n",
      "Epoch 28/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.7305 - loss: 34.3407 - reconstruction_loss: 26.8798 - val_kl_loss: 3.7923 - val_loss: 35.1477 - val_reconstruction_loss: 27.5631\n",
      "Epoch 29/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 3.6701 - loss: 34.4762 - reconstruction_loss: 27.1359 - val_kl_loss: 3.6519 - val_loss: 35.1402 - val_reconstruction_loss: 27.8365\n",
      "Epoch 30/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - kl_loss: 3.6933 - loss: 34.2907 - reconstruction_loss: 26.9040 - val_kl_loss: 3.7624 - val_loss: 34.7788 - val_reconstruction_loss: 27.2540\n",
      "Epoch 31/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 3.7550 - loss: 34.2920 - reconstruction_loss: 26.7821 - val_kl_loss: 3.7056 - val_loss: 34.9016 - val_reconstruction_loss: 27.4905\n",
      "Epoch 31: early stopping\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Best val_reconstruction_loss for this config: 27.2538\n",
      "*** New best model found! ***\n",
      "\n",
      "=== Training config 3/5 ===\n",
      "Config: {'learning_rate': 0.0001, 'batch_size': 64, 'latent_dim': 2, 'kl_reg': 0.5, 'optimizer': 'rmsprop', 'filter_base': 32, 'dense_units': 16, 'patience': 5, 'min_delta': 0.0001, 'restore_best_weights': False, 'monitor': 'val_loss'}\n",
      "Epoch 1/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - kl_loss: 0.1126 - loss: 118.3886 - reconstruction_loss: 118.3323 - val_kl_loss: 0.2923 - val_loss: 91.0451 - val_reconstruction_loss: 90.8989\n",
      "Epoch 2/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 0.9976 - loss: 82.6043 - reconstruction_loss: 82.1056 - val_kl_loss: 3.5393 - val_loss: 63.5585 - val_reconstruction_loss: 61.7888\n",
      "Epoch 3/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - kl_loss: 4.3044 - loss: 59.3260 - reconstruction_loss: 57.1738 - val_kl_loss: 5.4481 - val_loss: 53.7945 - val_reconstruction_loss: 51.0705\n",
      "Epoch 4/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 5.7970 - loss: 51.7967 - reconstruction_loss: 48.8982 - val_kl_loss: 6.1113 - val_loss: 48.3696 - val_reconstruction_loss: 45.3140\n",
      "Epoch 5/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 6.2800 - loss: 47.6888 - reconstruction_loss: 44.5488 - val_kl_loss: 6.5539 - val_loss: 45.7315 - val_reconstruction_loss: 42.4546\n",
      "Epoch 6/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - kl_loss: 6.6081 - loss: 45.2844 - reconstruction_loss: 41.9804 - val_kl_loss: 6.9536 - val_loss: 43.8653 - val_reconstruction_loss: 40.3885\n",
      "Epoch 7/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 6.6558 - loss: 43.0516 - reconstruction_loss: 39.7237 - val_kl_loss: 7.2131 - val_loss: 42.1472 - val_reconstruction_loss: 38.5407\n",
      "Epoch 8/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 6.7363 - loss: 41.6950 - reconstruction_loss: 38.3268 - val_kl_loss: 7.0730 - val_loss: 41.4267 - val_reconstruction_loss: 37.8902\n",
      "Epoch 9/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 6.8366 - loss: 40.6718 - reconstruction_loss: 37.2535 - val_kl_loss: 7.3581 - val_loss: 39.6469 - val_reconstruction_loss: 35.9678\n",
      "Epoch 10/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 6.8808 - loss: 39.4955 - reconstruction_loss: 36.0551 - val_kl_loss: 7.3061 - val_loss: 38.9651 - val_reconstruction_loss: 35.3120\n",
      "Epoch 11/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - kl_loss: 6.9481 - loss: 38.5762 - reconstruction_loss: 35.1021 - val_kl_loss: 7.3644 - val_loss: 38.5651 - val_reconstruction_loss: 34.8829\n",
      "Epoch 12/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 6.9837 - loss: 37.8427 - reconstruction_loss: 34.3508 - val_kl_loss: 7.7566 - val_loss: 38.0442 - val_reconstruction_loss: 34.1659\n",
      "Epoch 13/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - kl_loss: 7.0781 - loss: 37.5110 - reconstruction_loss: 33.9720 - val_kl_loss: 7.4788 - val_loss: 37.0270 - val_reconstruction_loss: 33.2876\n",
      "Epoch 14/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 7.0810 - loss: 37.1497 - reconstruction_loss: 33.6091 - val_kl_loss: 7.3350 - val_loss: 36.7590 - val_reconstruction_loss: 33.0915\n",
      "Epoch 15/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 7.0596 - loss: 36.7015 - reconstruction_loss: 33.1717 - val_kl_loss: 7.2663 - val_loss: 36.5339 - val_reconstruction_loss: 32.9007\n",
      "Epoch 16/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - kl_loss: 7.0795 - loss: 36.3712 - reconstruction_loss: 32.8314 - val_kl_loss: 7.2856 - val_loss: 36.5441 - val_reconstruction_loss: 32.9013\n",
      "Epoch 17/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 6.9870 - loss: 36.0734 - reconstruction_loss: 32.5798 - val_kl_loss: 7.1972 - val_loss: 35.6866 - val_reconstruction_loss: 32.0880\n",
      "Epoch 18/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 6.9450 - loss: 35.5718 - reconstruction_loss: 32.0993 - val_kl_loss: 7.1164 - val_loss: 35.5136 - val_reconstruction_loss: 31.9554\n",
      "Epoch 19/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 6.8465 - loss: 35.0683 - reconstruction_loss: 31.6450 - val_kl_loss: 7.1048 - val_loss: 35.6477 - val_reconstruction_loss: 32.0953\n",
      "Epoch 20/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - kl_loss: 6.7426 - loss: 34.8479 - reconstruction_loss: 31.4766 - val_kl_loss: 6.9850 - val_loss: 34.8098 - val_reconstruction_loss: 31.3174\n",
      "Epoch 21/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 6.7508 - loss: 34.5270 - reconstruction_loss: 31.1516 - val_kl_loss: 6.9532 - val_loss: 34.1128 - val_reconstruction_loss: 30.6362\n",
      "Epoch 22/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - kl_loss: 6.7306 - loss: 34.1219 - reconstruction_loss: 30.7565 - val_kl_loss: 6.8898 - val_loss: 34.1149 - val_reconstruction_loss: 30.6700\n",
      "Epoch 23/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - kl_loss: 6.6939 - loss: 33.7906 - reconstruction_loss: 30.4436 - val_kl_loss: 6.8278 - val_loss: 33.9326 - val_reconstruction_loss: 30.5187\n",
      "Epoch 24/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - kl_loss: 6.6015 - loss: 33.5261 - reconstruction_loss: 30.2254 - val_kl_loss: 6.8013 - val_loss: 33.4631 - val_reconstruction_loss: 30.0624\n",
      "Epoch 25/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - kl_loss: 6.5441 - loss: 33.2215 - reconstruction_loss: 29.9495 - val_kl_loss: 6.7240 - val_loss: 33.7664 - val_reconstruction_loss: 30.4044\n",
      "Epoch 26/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - kl_loss: 6.4893 - loss: 33.0175 - reconstruction_loss: 29.7729 - val_kl_loss: 6.5513 - val_loss: 32.9109 - val_reconstruction_loss: 29.6352\n",
      "Epoch 27/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - kl_loss: 6.4619 - loss: 32.7753 - reconstruction_loss: 29.5443 - val_kl_loss: 6.7123 - val_loss: 33.2252 - val_reconstruction_loss: 29.8691\n",
      "Epoch 28/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - kl_loss: 6.4415 - loss: 32.6247 - reconstruction_loss: 29.4040 - val_kl_loss: 6.5486 - val_loss: 32.4044 - val_reconstruction_loss: 29.1300\n",
      "Epoch 29/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - kl_loss: 6.4011 - loss: 32.2703 - reconstruction_loss: 29.0698 - val_kl_loss: 6.5111 - val_loss: 32.8313 - val_reconstruction_loss: 29.5758\n",
      "Epoch 30/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 99ms/step - kl_loss: 6.3417 - loss: 32.2065 - reconstruction_loss: 29.0357 - val_kl_loss: 6.4543 - val_loss: 32.4898 - val_reconstruction_loss: 29.2627\n",
      "Epoch 31/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - kl_loss: 6.2809 - loss: 32.0696 - reconstruction_loss: 28.9292 - val_kl_loss: 6.4574 - val_loss: 32.3988 - val_reconstruction_loss: 29.1701\n",
      "Epoch 32/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - kl_loss: 6.2628 - loss: 31.7949 - reconstruction_loss: 28.6635 - val_kl_loss: 6.4343 - val_loss: 32.4356 - val_reconstruction_loss: 29.2185\n",
      "Epoch 33/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - kl_loss: 6.1975 - loss: 31.5938 - reconstruction_loss: 28.4951 - val_kl_loss: 6.2755 - val_loss: 31.5882 - val_reconstruction_loss: 28.4504\n",
      "Epoch 34/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - kl_loss: 6.1692 - loss: 31.4337 - reconstruction_loss: 28.3491 - val_kl_loss: 6.2240 - val_loss: 31.5353 - val_reconstruction_loss: 28.4233\n",
      "Epoch 35/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - kl_loss: 6.1447 - loss: 31.3171 - reconstruction_loss: 28.2448 - val_kl_loss: 6.1306 - val_loss: 31.4582 - val_reconstruction_loss: 28.3929\n",
      "Epoch 36/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - kl_loss: 6.0709 - loss: 31.0626 - reconstruction_loss: 28.0272 - val_kl_loss: 6.1716 - val_loss: 31.2290 - val_reconstruction_loss: 28.1432\n",
      "Epoch 37/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 102ms/step - kl_loss: 6.0568 - loss: 31.0992 - reconstruction_loss: 28.0709 - val_kl_loss: 6.1965 - val_loss: 31.1545 - val_reconstruction_loss: 28.0563\n",
      "Epoch 38/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 98ms/step - kl_loss: 6.0031 - loss: 30.8114 - reconstruction_loss: 27.8099 - val_kl_loss: 6.0215 - val_loss: 31.2701 - val_reconstruction_loss: 28.2593\n",
      "Epoch 39/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - kl_loss: 5.9914 - loss: 30.9060 - reconstruction_loss: 27.9103 - val_kl_loss: 6.1436 - val_loss: 31.7792 - val_reconstruction_loss: 28.7074\n",
      "Epoch 40/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - kl_loss: 5.9708 - loss: 30.6602 - reconstruction_loss: 27.6748 - val_kl_loss: 6.0013 - val_loss: 31.0926 - val_reconstruction_loss: 28.0919\n",
      "Epoch 41/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - kl_loss: 5.9287 - loss: 30.5715 - reconstruction_loss: 27.6071 - val_kl_loss: 6.1285 - val_loss: 31.8359 - val_reconstruction_loss: 28.7716\n",
      "Epoch 42/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - kl_loss: 5.8831 - loss: 30.5285 - reconstruction_loss: 27.5869 - val_kl_loss: 5.8626 - val_loss: 31.0314 - val_reconstruction_loss: 28.1001\n",
      "Epoch 43/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - kl_loss: 5.8564 - loss: 30.3777 - reconstruction_loss: 27.4494 - val_kl_loss: 6.0607 - val_loss: 30.9568 - val_reconstruction_loss: 27.9264\n",
      "Epoch 44/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - kl_loss: 5.8347 - loss: 30.2832 - reconstruction_loss: 27.3658 - val_kl_loss: 5.8505 - val_loss: 30.3922 - val_reconstruction_loss: 27.4669\n",
      "Epoch 45/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - kl_loss: 5.8131 - loss: 30.1412 - reconstruction_loss: 27.2346 - val_kl_loss: 5.8325 - val_loss: 30.8442 - val_reconstruction_loss: 27.9279\n",
      "Epoch 46/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - kl_loss: 5.7479 - loss: 30.1245 - reconstruction_loss: 27.2505 - val_kl_loss: 5.8312 - val_loss: 30.7535 - val_reconstruction_loss: 27.8379\n",
      "Epoch 47/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - kl_loss: 5.7391 - loss: 30.0220 - reconstruction_loss: 27.1525 - val_kl_loss: 5.7493 - val_loss: 30.5420 - val_reconstruction_loss: 27.6673\n",
      "Epoch 48/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - kl_loss: 5.7419 - loss: 30.0830 - reconstruction_loss: 27.2120 - val_kl_loss: 5.8925 - val_loss: 30.6094 - val_reconstruction_loss: 27.6632\n",
      "Epoch 49/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - kl_loss: 5.7437 - loss: 29.9148 - reconstruction_loss: 27.0429 - val_kl_loss: 5.8749 - val_loss: 30.9618 - val_reconstruction_loss: 28.0243\n",
      "Epoch 49: early stopping\n",
      "Best val_loss for this config: 30.3922\n",
      "\n",
      "=== Training config 4/5 ===\n",
      "Config: {'learning_rate': 0.0001, 'batch_size': 128, 'latent_dim': 2, 'kl_reg': 0.5, 'optimizer': 'rmsprop', 'filter_base': 32, 'dense_units': 32, 'patience': 3, 'min_delta': 0.0001, 'restore_best_weights': True, 'monitor': 'val_reconstruction_loss'}\n",
      "Epoch 1/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 173ms/step - kl_loss: 0.7223 - loss: 121.7066 - reconstruction_loss: 121.3454 - val_kl_loss: 1.7824 - val_loss: 97.2957 - val_reconstruction_loss: 96.4045\n",
      "Epoch 2/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 155ms/step - kl_loss: 2.6495 - loss: 90.3068 - reconstruction_loss: 88.9821 - val_kl_loss: 4.0610 - val_loss: 75.2605 - val_reconstruction_loss: 73.2300\n",
      "Epoch 3/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - kl_loss: 4.3171 - loss: 72.5646 - reconstruction_loss: 70.4061 - val_kl_loss: 3.8235 - val_loss: 67.1558 - val_reconstruction_loss: 65.2440\n",
      "Epoch 4/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - kl_loss: 4.1636 - loss: 65.4432 - reconstruction_loss: 63.3614 - val_kl_loss: 4.2608 - val_loss: 61.4777 - val_reconstruction_loss: 59.3473\n",
      "Epoch 5/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 155ms/step - kl_loss: 4.3227 - loss: 59.5073 - reconstruction_loss: 57.3459 - val_kl_loss: 4.2474 - val_loss: 53.0440 - val_reconstruction_loss: 50.9203\n",
      "Epoch 6/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 156ms/step - kl_loss: 4.6359 - loss: 51.3822 - reconstruction_loss: 49.0643 - val_kl_loss: 4.8711 - val_loss: 46.2011 - val_reconstruction_loss: 43.7655\n",
      "Epoch 7/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - kl_loss: 5.0413 - loss: 45.5708 - reconstruction_loss: 43.0501 - val_kl_loss: 5.0863 - val_loss: 42.9488 - val_reconstruction_loss: 40.4056\n",
      "Epoch 8/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - kl_loss: 5.2547 - loss: 42.7761 - reconstruction_loss: 40.1488 - val_kl_loss: 4.8954 - val_loss: 42.4293 - val_reconstruction_loss: 39.9816\n",
      "Epoch 9/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - kl_loss: 5.2877 - loss: 41.4686 - reconstruction_loss: 38.8247 - val_kl_loss: 5.4265 - val_loss: 40.0072 - val_reconstruction_loss: 37.2939\n",
      "Epoch 10/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 156ms/step - kl_loss: 5.4354 - loss: 40.2366 - reconstruction_loss: 37.5189 - val_kl_loss: 5.4096 - val_loss: 38.8494 - val_reconstruction_loss: 36.1446\n",
      "Epoch 11/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - kl_loss: 5.4318 - loss: 39.2693 - reconstruction_loss: 36.5534 - val_kl_loss: 5.5813 - val_loss: 38.5564 - val_reconstruction_loss: 35.7658\n",
      "Epoch 12/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 147ms/step - kl_loss: 5.4788 - loss: 38.7679 - reconstruction_loss: 36.0285 - val_kl_loss: 5.0535 - val_loss: 40.4691 - val_reconstruction_loss: 37.9424\n",
      "Epoch 13/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - kl_loss: 5.4013 - loss: 38.5763 - reconstruction_loss: 35.8757 - val_kl_loss: 5.2434 - val_loss: 38.0218 - val_reconstruction_loss: 35.4001\n",
      "Epoch 14/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 169ms/step - kl_loss: 5.4696 - loss: 38.2878 - reconstruction_loss: 35.5530 - val_kl_loss: 5.4753 - val_loss: 36.9428 - val_reconstruction_loss: 34.2052\n",
      "Epoch 15/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 156ms/step - kl_loss: 5.4675 - loss: 37.4302 - reconstruction_loss: 34.6964 - val_kl_loss: 5.1138 - val_loss: 38.9788 - val_reconstruction_loss: 36.4219\n",
      "Epoch 16/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - kl_loss: 5.3956 - loss: 37.4152 - reconstruction_loss: 34.7174 - val_kl_loss: 5.3831 - val_loss: 36.2559 - val_reconstruction_loss: 33.5644\n",
      "Epoch 17/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - kl_loss: 5.3890 - loss: 36.7191 - reconstruction_loss: 34.0246 - val_kl_loss: 5.5392 - val_loss: 37.0368 - val_reconstruction_loss: 34.2672\n",
      "Epoch 18/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 149ms/step - kl_loss: 5.3951 - loss: 36.6285 - reconstruction_loss: 33.9310 - val_kl_loss: 5.0650 - val_loss: 37.6180 - val_reconstruction_loss: 35.0855\n",
      "Epoch 19/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - kl_loss: 5.3024 - loss: 36.5867 - reconstruction_loss: 33.9355 - val_kl_loss: 5.4023 - val_loss: 35.6860 - val_reconstruction_loss: 32.9849\n",
      "Epoch 20/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - kl_loss: 5.3635 - loss: 36.0043 - reconstruction_loss: 33.3225 - val_kl_loss: 5.1339 - val_loss: 36.1467 - val_reconstruction_loss: 33.5797\n",
      "Epoch 21/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - kl_loss: 5.3235 - loss: 36.0108 - reconstruction_loss: 33.3491 - val_kl_loss: 5.4620 - val_loss: 35.3848 - val_reconstruction_loss: 32.6538\n",
      "Epoch 22/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - kl_loss: 5.3778 - loss: 35.6346 - reconstruction_loss: 32.9457 - val_kl_loss: 5.2461 - val_loss: 35.0920 - val_reconstruction_loss: 32.4689\n",
      "Epoch 23/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - kl_loss: 5.3297 - loss: 36.3038 - reconstruction_loss: 33.6389 - val_kl_loss: 5.3498 - val_loss: 34.8043 - val_reconstruction_loss: 32.1294\n",
      "Epoch 24/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - kl_loss: 5.3198 - loss: 35.2495 - reconstruction_loss: 32.5896 - val_kl_loss: 5.3279 - val_loss: 34.8311 - val_reconstruction_loss: 32.1671\n",
      "Epoch 25/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - kl_loss: 5.2841 - loss: 35.2136 - reconstruction_loss: 32.5715 - val_kl_loss: 5.0306 - val_loss: 35.5120 - val_reconstruction_loss: 32.9967\n",
      "Epoch 26/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - kl_loss: 5.2206 - loss: 35.1184 - reconstruction_loss: 32.5081 - val_kl_loss: 5.3369 - val_loss: 34.8146 - val_reconstruction_loss: 32.1461\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Best val_reconstruction_loss for this config: 32.1294\n",
      "\n",
      "=== Training config 5/5 ===\n",
      "Config: {'learning_rate': 0.0001, 'batch_size': 128, 'latent_dim': 8, 'kl_reg': 0.5, 'optimizer': 'rmsprop', 'filter_base': 16, 'dense_units': 16, 'patience': 3, 'min_delta': 0.001, 'restore_best_weights': False, 'monitor': 'val_reconstruction_loss'}\n",
      "Epoch 1/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - kl_loss: 0.2345 - loss: 122.6573 - reconstruction_loss: 122.5400 - val_kl_loss: 1.0076 - val_loss: 106.9428 - val_reconstruction_loss: 106.4390\n",
      "Epoch 2/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 1.0332 - loss: 103.3897 - reconstruction_loss: 102.8732 - val_kl_loss: 1.4722 - val_loss: 95.9669 - val_reconstruction_loss: 95.2308\n",
      "Epoch 3/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 2.1772 - loss: 91.6146 - reconstruction_loss: 90.5260 - val_kl_loss: 4.3858 - val_loss: 81.3894 - val_reconstruction_loss: 79.1965\n",
      "Epoch 4/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - kl_loss: 5.0343 - loss: 78.8573 - reconstruction_loss: 76.3402 - val_kl_loss: 5.2987 - val_loss: 71.9822 - val_reconstruction_loss: 69.3329\n",
      "Epoch 5/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - kl_loss: 5.6682 - loss: 69.4230 - reconstruction_loss: 66.5890 - val_kl_loss: 5.8732 - val_loss: 62.6553 - val_reconstruction_loss: 59.7187\n",
      "Epoch 6/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 6.5923 - loss: 60.3643 - reconstruction_loss: 57.0682 - val_kl_loss: 8.0617 - val_loss: 54.9283 - val_reconstruction_loss: 50.8974\n",
      "Epoch 7/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - kl_loss: 8.3530 - loss: 52.8428 - reconstruction_loss: 48.6664 - val_kl_loss: 9.6182 - val_loss: 49.3050 - val_reconstruction_loss: 44.4959\n",
      "Epoch 8/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - kl_loss: 9.7722 - loss: 47.9081 - reconstruction_loss: 43.0220 - val_kl_loss: 10.5648 - val_loss: 45.4556 - val_reconstruction_loss: 40.1732\n",
      "Epoch 9/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - kl_loss: 10.8848 - loss: 44.1726 - reconstruction_loss: 38.7302 - val_kl_loss: 11.6040 - val_loss: 42.3074 - val_reconstruction_loss: 36.5054\n",
      "Epoch 10/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 11.4971 - loss: 41.6551 - reconstruction_loss: 35.9066 - val_kl_loss: 12.1049 - val_loss: 40.2251 - val_reconstruction_loss: 34.1726\n",
      "Epoch 11/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 11.8510 - loss: 39.7476 - reconstruction_loss: 33.8221 - val_kl_loss: 12.4518 - val_loss: 38.7539 - val_reconstruction_loss: 32.5280\n",
      "Epoch 12/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - kl_loss: 12.0304 - loss: 38.1919 - reconstruction_loss: 32.1767 - val_kl_loss: 11.9275 - val_loss: 37.6253 - val_reconstruction_loss: 31.6615\n",
      "Epoch 13/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - kl_loss: 11.9211 - loss: 36.8423 - reconstruction_loss: 30.8818 - val_kl_loss: 12.0903 - val_loss: 36.4489 - val_reconstruction_loss: 30.4038\n",
      "Epoch 14/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 11.9760 - loss: 35.9208 - reconstruction_loss: 29.9329 - val_kl_loss: 12.0493 - val_loss: 35.3486 - val_reconstruction_loss: 29.3239\n",
      "Epoch 15/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - kl_loss: 11.9145 - loss: 34.7789 - reconstruction_loss: 28.8217 - val_kl_loss: 12.0603 - val_loss: 34.3775 - val_reconstruction_loss: 28.3474\n",
      "Epoch 16/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 11.9206 - loss: 34.0990 - reconstruction_loss: 28.1387 - val_kl_loss: 12.0336 - val_loss: 33.9400 - val_reconstruction_loss: 27.9232\n",
      "Epoch 17/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - kl_loss: 11.9573 - loss: 33.3161 - reconstruction_loss: 27.3375 - val_kl_loss: 11.9883 - val_loss: 32.9791 - val_reconstruction_loss: 26.9849\n",
      "Epoch 18/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - kl_loss: 11.9805 - loss: 32.8593 - reconstruction_loss: 26.8690 - val_kl_loss: 12.0323 - val_loss: 32.4031 - val_reconstruction_loss: 26.3870\n",
      "Epoch 19/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - kl_loss: 11.9785 - loss: 32.2117 - reconstruction_loss: 26.2225 - val_kl_loss: 11.9173 - val_loss: 31.9698 - val_reconstruction_loss: 26.0112\n",
      "Epoch 20/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 11.9213 - loss: 31.7987 - reconstruction_loss: 25.8380 - val_kl_loss: 11.9942 - val_loss: 31.4157 - val_reconstruction_loss: 25.4185\n",
      "Epoch 21/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 11.8763 - loss: 31.3855 - reconstruction_loss: 25.4473 - val_kl_loss: 11.8271 - val_loss: 31.3905 - val_reconstruction_loss: 25.4769\n",
      "Epoch 22/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 11.8237 - loss: 31.0501 - reconstruction_loss: 25.1383 - val_kl_loss: 11.9053 - val_loss: 31.0364 - val_reconstruction_loss: 25.0837\n",
      "Epoch 23/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 11.8965 - loss: 30.7599 - reconstruction_loss: 24.8117 - val_kl_loss: 11.9326 - val_loss: 30.4632 - val_reconstruction_loss: 24.4969\n",
      "Epoch 24/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 11.8006 - loss: 30.3783 - reconstruction_loss: 24.4781 - val_kl_loss: 11.8662 - val_loss: 30.3575 - val_reconstruction_loss: 24.4243\n",
      "Epoch 25/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - kl_loss: 11.7577 - loss: 30.1541 - reconstruction_loss: 24.2753 - val_kl_loss: 11.7341 - val_loss: 29.9768 - val_reconstruction_loss: 24.1098\n",
      "Epoch 26/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - kl_loss: 11.7094 - loss: 29.7580 - reconstruction_loss: 23.9033 - val_kl_loss: 11.8882 - val_loss: 29.9530 - val_reconstruction_loss: 24.0089\n",
      "Epoch 27/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 11.6846 - loss: 29.6645 - reconstruction_loss: 23.8222 - val_kl_loss: 11.9322 - val_loss: 30.1081 - val_reconstruction_loss: 24.1419\n",
      "Epoch 28/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - kl_loss: 11.7085 - loss: 29.4773 - reconstruction_loss: 23.6230 - val_kl_loss: 11.7427 - val_loss: 29.1838 - val_reconstruction_loss: 23.3125\n",
      "Epoch 29/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - kl_loss: 11.6951 - loss: 29.1993 - reconstruction_loss: 23.3518 - val_kl_loss: 11.6082 - val_loss: 29.9087 - val_reconstruction_loss: 24.1046\n",
      "Epoch 30/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - kl_loss: 11.6754 - loss: 29.2395 - reconstruction_loss: 23.4018 - val_kl_loss: 11.6387 - val_loss: 29.0738 - val_reconstruction_loss: 23.2545\n",
      "Epoch 31/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 11.5996 - loss: 28.7237 - reconstruction_loss: 22.9238 - val_kl_loss: 11.6365 - val_loss: 28.7860 - val_reconstruction_loss: 22.9678\n",
      "Epoch 32/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 11.5755 - loss: 28.5917 - reconstruction_loss: 22.8040 - val_kl_loss: 11.5907 - val_loss: 28.7743 - val_reconstruction_loss: 22.9790\n",
      "Epoch 33/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - kl_loss: 11.5686 - loss: 28.5689 - reconstruction_loss: 22.7845 - val_kl_loss: 11.7806 - val_loss: 28.5845 - val_reconstruction_loss: 22.6942\n",
      "Epoch 34/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - kl_loss: 11.5643 - loss: 28.3282 - reconstruction_loss: 22.5460 - val_kl_loss: 11.5596 - val_loss: 28.4745 - val_reconstruction_loss: 22.6947\n",
      "Epoch 35/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - kl_loss: 11.4902 - loss: 28.2275 - reconstruction_loss: 22.4824 - val_kl_loss: 11.6213 - val_loss: 28.2461 - val_reconstruction_loss: 22.4355\n",
      "Epoch 36/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - kl_loss: 11.5157 - loss: 28.1883 - reconstruction_loss: 22.4305 - val_kl_loss: 11.4906 - val_loss: 28.3662 - val_reconstruction_loss: 22.6209\n",
      "Epoch 37/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - kl_loss: 11.4518 - loss: 28.0074 - reconstruction_loss: 22.2815 - val_kl_loss: 11.4125 - val_loss: 28.3924 - val_reconstruction_loss: 22.6862\n",
      "Epoch 38/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - kl_loss: 11.4353 - loss: 27.8646 - reconstruction_loss: 22.1470 - val_kl_loss: 11.6152 - val_loss: 28.0412 - val_reconstruction_loss: 22.2335\n",
      "Epoch 39/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 11.4551 - loss: 27.7903 - reconstruction_loss: 22.0628 - val_kl_loss: 11.5109 - val_loss: 27.6676 - val_reconstruction_loss: 21.9122\n",
      "Epoch 40/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - kl_loss: 11.4600 - loss: 27.5732 - reconstruction_loss: 21.8432 - val_kl_loss: 11.5030 - val_loss: 27.5029 - val_reconstruction_loss: 21.7515\n",
      "Epoch 41/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - kl_loss: 11.4527 - loss: 27.3771 - reconstruction_loss: 21.6508 - val_kl_loss: 11.5370 - val_loss: 27.7147 - val_reconstruction_loss: 21.9462\n",
      "Epoch 42/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 11.4134 - loss: 27.2561 - reconstruction_loss: 21.5494 - val_kl_loss: 11.6238 - val_loss: 27.5936 - val_reconstruction_loss: 21.7817\n",
      "Epoch 43/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - kl_loss: 11.4358 - loss: 27.2083 - reconstruction_loss: 21.4904 - val_kl_loss: 11.3386 - val_loss: 27.9111 - val_reconstruction_loss: 22.2418\n",
      "Epoch 43: early stopping\n",
      "Best val_reconstruction_loss for this config: 21.7515\n",
      "*** New best model found! ***\n",
      "\n",
      "=== Best Results Summary ===\n",
      "Best validation loss: 21.7515\n",
      "Best configuration: {'learning_rate': 0.0001, 'batch_size': 128, 'latent_dim': 8, 'kl_reg': 0.5, 'optimizer': 'rmsprop', 'filter_base': 16, 'dense_units': 16, 'patience': 3, 'min_delta': 0.001, 'restore_best_weights': False, 'monitor': 'val_reconstruction_loss'}\n"
     ]
    }
   ],
   "source": [
    "best_config = random_search_vae(\n",
    "    VAE_MODEL=VAE,\n",
    "    INPUT_SHAPE=INPUT_SHAPE,\n",
    "    param_grid=param_grid,\n",
    "    x_train=x_train,\n",
    "    x_val=x_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Training\n",
    "\n",
    "Using the best hyperparameters found, we instantiate and train a final model on the entire training dataset for a larger number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final Model with Best Hyperparameters ---\n",
      "Config: {'learning_rate': 0.0001, 'batch_size': 128, 'latent_dim': 8, 'kl_reg': 0.5, 'optimizer': 'rmsprop', 'filter_base': 16, 'dense_units': 16, 'patience': 3, 'min_delta': 0.001, 'restore_best_weights': False, 'monitor': 'val_reconstruction_loss'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_42      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ input_layer_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_105[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_42    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ max_pooling2d_42… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_43    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1568</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_43… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,104</span> │ flatten_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │ dense_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_log_var (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │ dense_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sampling_21         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ z_mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sampling</span>)          │                   │            │ z_log_var[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_42      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_105 (\u001b[38;5;33mConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │        \u001b[38;5;34m160\u001b[0m │ input_layer_42[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_105[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_42    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_106 (\u001b[38;5;33mConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │      \u001b[38;5;34m4,640\u001b[0m │ max_pooling2d_42… │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_43    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1568\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_43… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │     \u001b[38;5;34m25,104\u001b[0m │ flatten_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_mean (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m136\u001b[0m │ dense_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_log_var (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m136\u001b[0m │ dense_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sampling_21         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ z_mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mSampling\u001b[0m)          │                   │            │ z_log_var[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,368</span> (118.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m30,368\u001b[0m (118.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,272</span> (118.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,272\u001b[0m (118.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> (384.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m96\u001b[0m (384.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1568</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_86          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_87          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">145</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_43 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1568\u001b[0m)           │        \u001b[38;5;34m14,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_21 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_42 (\u001b[38;5;33mUpSampling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_107 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_86          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_43 (\u001b[38;5;33mUpSampling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_108 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m4,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_87          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_109 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │           \u001b[38;5;34m145\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,321</span> (110.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,321\u001b[0m (110.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,225</span> (110.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,225\u001b[0m (110.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> (384.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m96\u001b[0m (384.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - kl_loss: 0.1273 - loss: 125.2077 - reconstruction_loss: 125.1440 - val_kl_loss: 1.0921 - val_loss: 105.8814 - val_reconstruction_loss: 105.3353\n",
      "Epoch 2/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - kl_loss: 1.1629 - loss: 103.2452 - reconstruction_loss: 102.6637 - val_kl_loss: 2.4802 - val_loss: 93.2205 - val_reconstruction_loss: 91.9804\n",
      "Epoch 3/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - kl_loss: 3.6271 - loss: 89.1877 - reconstruction_loss: 87.3741 - val_kl_loss: 6.1845 - val_loss: 76.6302 - val_reconstruction_loss: 73.5380\n",
      "Epoch 4/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - kl_loss: 7.0449 - loss: 72.6679 - reconstruction_loss: 69.1455 - val_kl_loss: 8.0260 - val_loss: 63.3882 - val_reconstruction_loss: 59.3752\n",
      "Epoch 5/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - kl_loss: 8.5778 - loss: 60.6579 - reconstruction_loss: 56.3690 - val_kl_loss: 8.5705 - val_loss: 55.4299 - val_reconstruction_loss: 51.1447\n",
      "Epoch 6/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - kl_loss: 8.9066 - loss: 52.7538 - reconstruction_loss: 48.3005 - val_kl_loss: 8.7350 - val_loss: 48.4338 - val_reconstruction_loss: 44.0663\n",
      "Epoch 7/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - kl_loss: 8.8603 - loss: 46.6445 - reconstruction_loss: 42.2143 - val_kl_loss: 8.6506 - val_loss: 43.6593 - val_reconstruction_loss: 39.3339\n",
      "Epoch 8/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - kl_loss: 8.9219 - loss: 42.1302 - reconstruction_loss: 37.6692 - val_kl_loss: 9.1013 - val_loss: 39.8894 - val_reconstruction_loss: 35.3387\n",
      "Epoch 9/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - kl_loss: 9.4023 - loss: 39.0979 - reconstruction_loss: 34.3968 - val_kl_loss: 9.8406 - val_loss: 37.8367 - val_reconstruction_loss: 32.9164\n",
      "Epoch 10/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - kl_loss: 9.9317 - loss: 37.1401 - reconstruction_loss: 32.1742 - val_kl_loss: 9.9875 - val_loss: 36.1141 - val_reconstruction_loss: 31.1204\n",
      "Epoch 11/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - kl_loss: 10.2939 - loss: 35.6805 - reconstruction_loss: 30.5336 - val_kl_loss: 10.2228 - val_loss: 34.9773 - val_reconstruction_loss: 29.8659\n",
      "Epoch 12/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - kl_loss: 10.5230 - loss: 34.3791 - reconstruction_loss: 29.1177 - val_kl_loss: 10.4015 - val_loss: 33.6037 - val_reconstruction_loss: 28.4030\n",
      "Epoch 13/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - kl_loss: 10.6452 - loss: 33.5064 - reconstruction_loss: 28.1839 - val_kl_loss: 10.4971 - val_loss: 33.3205 - val_reconstruction_loss: 28.0720\n",
      "Epoch 14/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - kl_loss: 10.8133 - loss: 32.7411 - reconstruction_loss: 27.3344 - val_kl_loss: 10.6635 - val_loss: 32.6184 - val_reconstruction_loss: 27.2867\n",
      "Epoch 15/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - kl_loss: 10.9149 - loss: 32.0559 - reconstruction_loss: 26.5985 - val_kl_loss: 10.6817 - val_loss: 32.0332 - val_reconstruction_loss: 26.6924\n",
      "Epoch 16/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - kl_loss: 10.8919 - loss: 31.3936 - reconstruction_loss: 25.9477 - val_kl_loss: 11.0061 - val_loss: 31.1557 - val_reconstruction_loss: 25.6526\n",
      "Epoch 17/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - kl_loss: 11.0089 - loss: 30.9975 - reconstruction_loss: 25.4930 - val_kl_loss: 10.6942 - val_loss: 30.9640 - val_reconstruction_loss: 25.6169\n",
      "Epoch 18/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - kl_loss: 10.9408 - loss: 30.5779 - reconstruction_loss: 25.1075 - val_kl_loss: 10.6549 - val_loss: 30.7634 - val_reconstruction_loss: 25.4359\n",
      "Epoch 19/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - kl_loss: 10.9042 - loss: 30.2173 - reconstruction_loss: 24.7652 - val_kl_loss: 10.7334 - val_loss: 30.3621 - val_reconstruction_loss: 24.9954\n",
      "Epoch 20/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - kl_loss: 10.9817 - loss: 29.9746 - reconstruction_loss: 24.4837 - val_kl_loss: 10.7863 - val_loss: 29.8813 - val_reconstruction_loss: 24.4882\n"
     ]
    }
   ],
   "source": [
    "# Imposta i migliori iperparametri trovati o usa i default se la ricerca non ha avuto successo\n",
    "if best_config is None:\n",
    "    print(\"Random search fallita o saltata. Uso valori di default.\")\n",
    "    best_config = {\n",
    "        'learning_rate': 1e-3,\n",
    "        'batch_size': 128,\n",
    "        'latent_dim': 2,\n",
    "        'kl_reg': 1.0,\n",
    "        'filter_base': 16,\n",
    "        'dense_units': 16,\n",
    "        'optimizer': 'adam',\n",
    "        'patience': 3,\n",
    "        'min_delta': 1e-4,\n",
    "        'restore_best_weights': True,\n",
    "        'monitor': 'val_loss'\n",
    "    }\n",
    "\n",
    "LATENT_DIM = best_config.get('latent_dim', 2)\n",
    "BATCH_SIZE = best_config.get('batch_size', 128)\n",
    "LEARNING_RATE = best_config.get('learning_rate', 1e-3)\n",
    "KL_REG = best_config.get('kl_reg', 1.0)\n",
    "FILTER_BASE = best_config.get('filter_base', 16)\n",
    "DENSE_UNITS = best_config.get('dense_units', 16)\n",
    "OPTIMIZER = best_config.get('optimizer', 'adam')\n",
    "EPOCHS = 20\n",
    "\n",
    "print(\"\\n--- Training Final Model with Best Hyperparameters ---\")\n",
    "print(f\"Config: {best_config}\")\n",
    "\n",
    "# Ricrea il dataset completo unendo train e validation\n",
    "import numpy as np\n",
    "x_train_full = np.concatenate([x_train, x_val], axis=0)\n",
    "\n",
    "# Crea encoder e decoder con la struttura aggiornata\n",
    "final_encoder = vae_enc(INPUT_SHAPE, LATENT_DIM, filter_base=FILTER_BASE, dense_units=DENSE_UNITS)\n",
    "final_decoder = vae_dec(LATENT_DIM, filter_base=FILTER_BASE, dense_units=DENSE_UNITS)\n",
    "final_vae = VAE(final_encoder, final_decoder, reg=KL_REG)\n",
    "\n",
    "if OPTIMIZER == 'adam':\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "elif OPTIMIZER == 'rmsprop':\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=LEARNING_RATE)\n",
    "else:\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "final_vae.compile(optimizer=optimizer)\n",
    "\n",
    "final_encoder.summary()\n",
    "final_decoder.summary()\n",
    "\n",
    "# EarlyStopping callback se richiesto\n",
    "callbacks = []\n",
    "if 'patience' in best_config and 'monitor' in best_config:\n",
    "    callbacks.append(\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=best_config['monitor'],\n",
    "            patience=best_config['patience'],\n",
    "            min_delta=best_config['min_delta'],\n",
    "            verbose=1,\n",
    "            restore_best_weights=best_config['restore_best_weights'],\n",
    "            mode='min'\n",
    "        )\n",
    "    )\n",
    "\n",
    "history = final_vae.fit(\n",
    "    x_train_full, x_train_full,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(x_test, x_test),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Assessment\n",
    "The reconstruction capabilities are evaluated both quantitatively and qualitatively.\n",
    "\n",
    "1.  **Quantitative Evaluation:** We use the trained model to reconstruct the test set images and calculate the **Mean Squared Error (MSE)** and **Mean Absolute Error (MAE)** between the original and reconstructed images. Lower values indicate better performance.\n",
    "2.  **Qualitative Evaluation:** We visually inspect the results by plotting original test images against their reconstructions. This helps assess the fidelity and sharpness of the output. We also visualize the generative latent space (if 2D) to confirm that the model has learned a smooth manifold for generating new, coherent images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\n",
      "Test Set Evaluation:\n",
      "Mean Squared Error (MSE): 0.0310\n",
      "\n",
      "--- Qualitative Evaluation ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantitative Evaluation of Reconstruction\n",
    "reconstructed_images = final_vae.predict(x_test)\n",
    "test_mse = mean_squared_error(x_test.flatten(), reconstructed_images.flatten())\n",
    "\n",
    "print(f\"\\nTest Set Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse:.4f}\")\n",
    "\n",
    "# Qualitative Evaluation\n",
    "def plot_reconstructions(original, reconstructed, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.suptitle(\"Reconstruction Evaluation: Original vs Reconstructed\")\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original[i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(\"Original\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(\"Reconstructed\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "def plot_latent_space(vae, n=15, figsize=15):\n",
    "    digit_size = 28\n",
    "    scale = 1.5\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = vae.decoder.predict(z_sample, verbose=0)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size : (i + 1) * digit_size, j * digit_size : (j + 1) * digit_size] = digit\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    plt.title(\"Generative Latent Space Exploration\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- Qualitative Evaluation ---\")\n",
    "plot_reconstructions(x_test, reconstructed_images)\n",
    "\n",
    "# The latent space plot is only meaningful for 2D latent spaces\n",
    "if LATENT_DIM == 2:\n",
    "    print(\"\\n--- Latent Space Visualization (for LATENT_DIM=2) ---\")\n",
    "    plot_latent_space(final_vae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
